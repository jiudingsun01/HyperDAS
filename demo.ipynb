{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28da06ea",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from src.hyperdas.data_utils import get_ravel_collate_fn\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "ravel_city = load_from_disk(\"experiments/RAVEL/data/city_train\")\n",
    "\n",
    "# Define the collate function\n",
    "collate_fn = get_ravel_collate_fn(\n",
    "    tokenizer, \n",
    "    source_suffix_visibility=False,  # Set to True to mask the part of the source sentence which contains the attribute for the HyperDAS \n",
    "    base_suffix_visibility=False,  # Set to True to mask the part of the base sentence which contains the attribute for the HyperDAS\n",
    "    add_space_before_target=True \n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    ravel_city, batch_size=2, collate_fn=collate_fn, shuffle=True\n",
    ")\n",
    "\n",
    "for batch in data_loader:\n",
    "    break\n",
    "\n",
    "print(batch)\n",
    "\n",
    "# the description of the intervention to apply, e.g. \"Interchange the country of the city mentioned in the sentence\"\n",
    "editor_input_ids = batch[\"editor_input_ids\"].to(\"cuda\") # torch.Size([2, 11]) the description of the intervention  \n",
    "\n",
    "# 1 or 0 to indicate if this intervention is intended to be causal. Artifacts of the RAVEL dataset\n",
    "is_causal = batch[\"is_causal\"].to(\"cuda\") # torch.Size([2])\n",
    "\n",
    "# The base sentence to be edited, e.g. \"Paris is the capital of France\"\n",
    "base_input_ids = batch[\"base_input_ids\"].to(\"cuda\") # torch.Size([2, 24])\n",
    "\n",
    "# The attention mask for the base sentence, 1 for real tokens, 0 for padding tokens\n",
    "base_attention_mask = batch[\"base_attention_mask\"].to(\"cuda\") # torch.Size([2, 24])\n",
    "\n",
    "# The intervention mask for the base sentence, 1 for tokens could be edited, 0 for tokens to be kept unchanged and unseen by the HyperDAS\n",
    "base_intervention_mask = batch[\"base_intervention_mask\"].to(\"cuda\") # torch.Size([2, 24])\n",
    "\n",
    "# The source sentence to extract the representation, e.g. \"Berlin is a lovely city!\"\n",
    "source_input_ids = batch[\"source_input_ids\"].to(\"cuda\") # torch.Size([2, 28])\n",
    "\n",
    "# The attention mask for the source sentence, 1 for real tokens, 0 for padding tokens\n",
    "source_attention_mask = batch[\"source_attention_mask\"].to(\"cuda\") # torch.Size([2, 28])\n",
    "\n",
    "# The intervention mask for the source sentence, 1 for tokens could be edited, 0 for tokens to be kept unchanged and unseen by the HyperDAS\n",
    "source_intervention_mask = batch[\"source_intervention_mask\"].to(\"cuda\") # torch.Size([2, 28])\n",
    "\n",
    "# The desired generation if the intervention is properly applied, e.g. \"Paris is the capital of Germany\"\n",
    "labels = batch[\"labels\"].to(\"cuda\") # torch.Size([2, 24])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13f692",
   "metadata": {},
   "source": [
    "### Initializing a Hypernetwork Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hyperdas.llama3.modules import LlamaInterpreterConfig, LlamaInterpreter\n",
    "\n",
    "config = LlamaInterpreterConfig.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "config.name_or_path = \"meta-llama/Meta-Llama-3-8B\"\n",
    "config.torch_dtype = torch.bfloat16\n",
    "\n",
    "config.num_editing_heads = 32 # the number of attention heads per layer for the HyperDAS\n",
    "config.chop_editor_at_layer = 4 # the number of layer for the HyperDAS\n",
    "config.intervention_layer = 20 # the layer of the target model to apply the intervention\n",
    "config._attn_implementation = 'eager'\n",
    "config.initialize_from_scratch = True # False to initialize the model from the pretrained Llama3-8B weight\n",
    "config.ablate_base_token_attention = False # True to ablate the attention of the base sentence (HyperDAS cannot access the information of the base sentence)\n",
    "config.ablate_source_token_attention = False # True to ablate the attention of the source sentence (HyperDAS cannot access the information of the source sentence)\n",
    "config.break_asymmetric = False # True to use the source-blinded attention mechanism when processing information from base or counterfactual sentences\n",
    "\n",
    "interpreter = LlamaInterpreter(\n",
    "    config, \n",
    "    subspace_module=\"ReflectSelect\", # The HouseHolder Transformation algorithm used in the paper\n",
    "    das_dimension=2, # the dimension of feature subspace for interchange intervention\n",
    ")\n",
    "\n",
    "interpreter = interpreter.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b5b27",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f6479",
   "metadata": {},
   "source": [
    "### Train your own HyperDAS on Llama3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1492001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sentences = [\n",
    "    \"Paris is the capital of France\", \n",
    "    \"I love eating tasty food and pizza is tasty, so I love pizza\"\n",
    "]\n",
    "\n",
    "source_sentences = [\n",
    "    \"Berlin is a lovely city!\",\n",
    "    \"Pizza tastes bad\"\n",
    "]\n",
    "\n",
    "intervention_instruction = [\n",
    "    \"Interchange the country of the city mentioned in the sentence\",\n",
    "    \"Interchange the taste of pizza\"\n",
    "]\n",
    "\n",
    "target_generation = [\n",
    "    \"Paris is the capital of Germany\",\n",
    "    \"I love eating tasty food and pizza is tasty, so I hate pizza\"\n",
    "]\n",
    "\n",
    "\n",
    "base_inputs = tokenizer(\n",
    "    base_sentences, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=32\n",
    ")\n",
    "\n",
    "source_inputs = tokenizer(\n",
    "    source_sentences, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=32\n",
    ")\n",
    "\n",
    "label_inputs = tokenizer(\n",
    "    target_generation, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=32\n",
    ")\n",
    "\n",
    "editor_inputs = tokenizer(\n",
    "    intervention_instruction, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=32\n",
    ")\n",
    "\n",
    "editor_input_ids = editor_inputs[\"input_ids\"].to(\"cuda\")\n",
    "base_input_ids = base_inputs[\"input_ids\"].to(\"cuda\")\n",
    "base_attention_mask = base_inputs[\"attention_mask\"].to(\"cuda\") # Set the intervention mask to be the same as the attention mask \n",
    "source_input_ids = source_inputs[\"input_ids\"].to(\"cuda\")\n",
    "source_attention_mask = source_inputs[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "base_intervention_mask = base_inputs[\"attention_mask\"].to(\"cuda\")\n",
    "base_intervention_mask[0, -1] = 0 # Mask the prediction 'France' for intervention\n",
    "base_intervention_mask[1, -2] = 0 # Mask the prediction 'love' for intervention\n",
    "base_intervention_mask[1, -1] = 0 # Mask the prediction 'pizza' for intervention\n",
    "\n",
    "source_intervention_mask = source_inputs[\"attention_mask\"].to(\"cuda\") # Set the intervention mask to be the same as the attention mask\n",
    "\n",
    "labels = label_inputs[\"input_ids\"].to(\"cuda\")\n",
    "labels[0][:-1] = -100 # Mask for loss\n",
    "labels[1][:-2] = -100 # Mask for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae020e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  12.463130950927734\n"
     ]
    }
   ],
   "source": [
    "output = interpreter(\n",
    "    base_input_ids=base_input_ids,\n",
    "    base_attention_mask=base_attention_mask,\n",
    "    base_intervention_mask=base_intervention_mask,\n",
    "    source_input_ids=source_input_ids,\n",
    "    source_attention_mask=source_attention_mask,\n",
    "    source_intervention_mask=source_intervention_mask,\n",
    "    editor_input_ids=editor_input_ids,\n",
    "    editor_attention_mask=editor_input_ids != tokenizer.eos_token_id,\n",
    "    output_intervention_weight=True,\n",
    ")\n",
    "\n",
    "logits = output[\"logits\"]\n",
    "intervention_matrix = output.intervention_weight # The selected token for intervention generated by the HyperDAS\n",
    "\n",
    "prediction = torch.argmax(logits, dim=-1) # Prediction of the target model after intervention applied by the HyperDAS\n",
    "\n",
    "log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "    logits.reshape(-1, logits.shape[-1]),\n",
    "    dim=1,\n",
    ")\n",
    "labels = labels.reshape(-1)\n",
    "\n",
    "assert labels.shape == log_prob_predictions.shape[:-1]\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "loss = criterion(log_prob_predictions, labels.long())\n",
    "\n",
    "print(\"Loss: \", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c4c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

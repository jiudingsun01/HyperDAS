{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get All Domains in Wikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from json import JSONDecodeError\n",
    "from time import sleep\n",
    "import re\n",
    "from urllib.parse import unquote, urlparse\n",
    "import openai\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_entites(qid, limit=10000):\n",
    "    \n",
    "    query = f'''\n",
    "        SELECT ?entity ?entityLabel\n",
    "        WHERE {{\n",
    "            ?entity wdt:P31 wd:{qid}.\n",
    "            ?entity rdfs:label ?entityLabel filter (lang(?entityLabel) = \"en\")\n",
    "        }}\n",
    "        LIMIT {limit}\n",
    "    '''\n",
    "    r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "    try:\n",
    "        data = r.json()\n",
    "    except JSONDecodeError:\n",
    "        sleep(5)\n",
    "        r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "        data = r.json()\n",
    "    \n",
    "    entities = dict()\n",
    "    \n",
    "    for e in data['results']['bindings']:\n",
    "        entity_id = e[\"entity\"]['value'].split('/')[-1]\n",
    "        entity_label = e[\"entityLabel\"]['value']\n",
    "        entities[entity_id] = entity_label\n",
    "        \n",
    "    return entities\n",
    "\n",
    "\n",
    "def get_entity_attributes(qid):\n",
    "    \n",
    "    query = f'''\n",
    "        SELECT ?property ?propertyLabel ?value ?valueLabel\n",
    "        WHERE {{\n",
    "            wd:{qid} ?p ?value .\n",
    "            ?property wikibase:directClaim ?p .\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "    '''\n",
    "    r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "    try:\n",
    "        data = r.json()\n",
    "    except JSONDecodeError:\n",
    "        sleep(5)\n",
    "        r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "        data = r.json()\n",
    "    \n",
    "    properties = dict()\n",
    "    \n",
    "    for p in data['results']['bindings']:\n",
    "        property_id = p[\"property\"]['value'].split('/')[-1]\n",
    "        property_label = p[\"propertyLabel\"]['value']\n",
    "        value_label = p[\"valueLabel\"]['value']\n",
    "        \n",
    "        if property_id.startswith('P'):\n",
    "            properties[property_id] = (property_label, value_label)\n",
    "                \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(url):\n",
    "    # Extracting the title from the URL\n",
    "    parsed_url = urlparse(url)\n",
    "    title = unquote(parsed_url.path.split('/')[-1])\n",
    "    \n",
    "    # API request setup\n",
    "    api_url = f'https://{parsed_url.netloc}/w/api.php'\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': title,\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': True,\n",
    "    }\n",
    "\n",
    "    # Making the request\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the page content\n",
    "    page = next(iter(data['query']['pages'].values()))\n",
    "    if 'extract' in page:\n",
    "        return page['extract']\n",
    "    else:\n",
    "        return 'Article content not found.'\n",
    "    \n",
    "def get_wikipedia_sentences(content, entity_name, word_upper_limit=20, word_lower_limit=5):\n",
    "    # Splitting the content into sentences by \". \", \".\\n\" or \".\\t\"\n",
    "    sentences = [s.strip() for s in re.split(r'\\.|\\;|\\,|\\n|\\!|\\?', content)]\n",
    "    \n",
    "    # Filtering the sentences that mention the entity\n",
    "    entity_sentences = [s for s in sentences if entity_name in s]\n",
    "    \n",
    "    # Filtering the sentences by word count\n",
    "    entity_sentences = [s for s in entity_sentences if len(s.split()) <= word_upper_limit and len(s.split()) >= word_lower_limit]\n",
    "    \n",
    "    return entity_sentences\n",
    "\n",
    "def get_wikipedia_url(wikidata_id, language='en'):\n",
    "    # Constructing the URL to call the API\n",
    "    url = f'https://www.wikidata.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'wbgetentities',\n",
    "        'ids': wikidata_id,\n",
    "        'format': 'json',\n",
    "        'props': 'sitelinks'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Accessing the sitelink for the specified language\n",
    "    try:\n",
    "        wikipedia_title = data['entities'][wikidata_id]['sitelinks'][f'{language}wiki']['title']\n",
    "        wikipedia_url = f\"https://{language}.wikipedia.org/wiki/{wikipedia_title.replace(' ', '_')}\"\n",
    "        return wikipedia_url\n",
    "    except KeyError:\n",
    "        return \"No Wikipedia article found for this language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "async def filter_attribute(attribute, retries=0):\n",
    "    \n",
    "    client = AsyncOpenAI(\n",
    "    # This is the default and can be omitted\n",
    "        api_key = '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'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        chat_completion = await client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Classify the following attribute into 'common knowledge' or 'specialized'.\\nExamples of common knowledge attribute: Cause of death, Nationality, category for people buried here, Country of origin\\nExamples of specialized attribute: image of grave, Canadiana Name Authority ID, Diamond Catalog ID for persons and organisations\\n\\n\\n\" + \"Attribute\" + attribute + \"\\n\\nClass: \"\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "    except openai.APIConnectionError as e:\n",
    "        if retries >= 5:\n",
    "            print(f\"Max retries reached. Skipping attribute {attribute}\")\n",
    "            return \"specialized\"\n",
    "        \n",
    "        print(f\"Timeout. Sleep for 5 seconds\")\n",
    "        asyncio.sleep(5)\n",
    "        return filter_attribute(attribute, retries + 1)\n",
    "        \n",
    "    # OpenAI API Key setup\n",
    "    content = chat_completion.choices[0].message.content\n",
    "    return content\n",
    "\n",
    "async def filter_attributes(attributes):\n",
    "    filtered_attributes = dict()\n",
    "    \n",
    "    # Prepare a list of tasks for asynchronous execution\n",
    "    tasks = [asyncio.create_task(filter_attribute(attributes[attribute][0])) for attribute in attributes]\n",
    "\n",
    "    # Use asyncio.gather to run tasks concurrently and wait for all to complete\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # After tasks complete, map results back to attribute keys\n",
    "    for attribute, result in zip(attributes, results):\n",
    "        filtered_attributes[attribute] = result\n",
    "        \n",
    "    is_common_knowledge = dict()\n",
    "    for attribute, pred in filtered_attributes.items():\n",
    "        is_common_knowledge[attribute] = \"specialized\" not in pred.lower()\n",
    "    \n",
    "    filtered_attributes = {k: attributes[k] for k, v in filtered_attributes.items() if is_common_knowledge[k]}\n",
    "        \n",
    "    filtered_attributes\n",
    "    return filtered_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "async def get_attribute_prompt(entity_name, context, attribute, label, retries=0):\n",
    "    \n",
    "    client = AsyncOpenAI(\n",
    "    # This is the default and can be omitted\n",
    "        api_key = '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'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "\n",
    "        chat_completion = await client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"Given a context sentence about {entity_name}, write a prompt continuing on the context and query the attribute which the model should predict the label in th next token.\"\n",
    "                },\n",
    "                # 1-st example\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Entity: Abraham Lincoln\\n\\nContext: Abraham Lincoln Association Archived April 28\\n\\nAttribute: cause of death\\n\\nLabel: shot to the head\\n\\n\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"Prompt: \\', the person who this association is named after was dead by \\'\"\n",
    "                },\n",
    "                # 2-nd example\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Entity: A Gang Story\\n\\nContext: A Gang Story (French: Les Lyonnais) is a 2011 French drama film \\n\\nAttribute: screenwriter\\n\\nLabel: Edgar Marie\\n\\n\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"Prompt: \\' which was brought to life by screenwriter \\'\"\n",
    "                },\n",
    "                # 3-rd example\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Entity: Palo Alto\\n\\nContext: This was also the case in Palo Alto and the surrounding areas\\n\\nAttribute: country\\n\\nLabel: United States of America\\n\\n\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"Prompt: \\', which is in the country of \\'\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Entity: {entity_name}\\n\\nContext: {context}\\n\\nAttribute: {attribute}\\n\\nLabel: {label}\\n\\n\"\n",
    "                },\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "        )\n",
    "    except openai.APIConnectionError as e:\n",
    "        \n",
    "        if retries >= 5:\n",
    "            print(f\"Max retries reached. Skipping attribute {attribute}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Timeout. Sleep for 5 seconds\")\n",
    "        asyncio.sleep(5)\n",
    "        return get_attribute_prompt(entity_name, context, attribute, label, retries + 1)\n",
    "        \n",
    "    # OpenAI API Key setup\n",
    "    content = chat_completion.choices[0].message.content\n",
    "    return content\n",
    "\n",
    "async def generate_prompts(entity_name, entity_contexts, attributes, labels):\n",
    "        \n",
    "    prompt_dict = dict()\n",
    "    \n",
    "    for context in entity_contexts:\n",
    "        prompt_dict[context] = dict()\n",
    "        \n",
    "        for attribute, label in zip(attributes, labels):\n",
    "            prompt = await get_attribute_prompt(entity_name, context, attribute, label)\n",
    "            \n",
    "            if prompt is None:\n",
    "                continue\n",
    "            prompt = prompt.replace(\"Prompt: \", \"\")[1:-1]\n",
    "            \n",
    "            prompt_dict[context][attribute] = (prompt, label)\n",
    "    \n",
    "    return prompt_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get Domain Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "import random\n",
    "\n",
    "async def generate_domain_dataset(qid, num_of_context=2, n_property=5, limit=2):\n",
    "    \n",
    "    domain_dataset = []\n",
    "    \n",
    "    all_entities = get_domain_entites(qid, limit)\n",
    "    \n",
    "    print(f\"Number of entities: {len(all_entities)}\")\n",
    "    \n",
    "    for entity_qid, entity_name in tqdm(all_entities.items()):\n",
    "        attributes = get_entity_attributes(entity_qid)\n",
    "        filtered_attributes = await filter_attributes(attributes)\n",
    "        \n",
    "        try:\n",
    "            wikipedia_url = get_wikipedia_url(entity_qid)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            wikipedia_content = get_wikipedia_content(wikipedia_url)\n",
    "        except requests.ConnectTimeout:\n",
    "            sleep(5)\n",
    "            wikipedia_content = get_wikipedia_content(wikipedia_url)\n",
    "            \n",
    "        entity_contexts = get_wikipedia_sentences(wikipedia_content, entity_name)\n",
    "        \n",
    "        if len(entity_contexts) > num_of_context:\n",
    "            entity_contexts = random.sample(entity_contexts, num_of_context)\n",
    "            \n",
    "        if len(entity_contexts) == 0 or len(filtered_attributes) == 0:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if len(filtered_attributes) > n_property:\n",
    "            selected_keys = random.sample(filtered_attributes.keys(), n_property)\n",
    "            filtered_attributes = {k: filtered_attributes[k] for k in selected_keys}\n",
    "        \n",
    "        attributes = []\n",
    "        labels = []\n",
    "        \n",
    "        for attribute_qid in filtered_attributes.keys():\n",
    "            attribute, value = filtered_attributes[attribute_qid]\n",
    "            attributes.append(attribute)\n",
    "            labels.append(value)\n",
    "            \n",
    "        prompt_dict = await generate_prompts(entity_name, entity_contexts, attributes, labels)\n",
    "        \n",
    "        for context in prompt_dict:\n",
    "            for attribute in prompt_dict[context].keys():\n",
    "                prompt, label = prompt_dict[context][attribute]\n",
    "                \n",
    "                domain_dataset.append({\n",
    "                    'entity': entity_name,\n",
    "                    'context': context,\n",
    "                    'attribute': attribute,\n",
    "                    'prompt': prompt,\n",
    "                    'label': label\n",
    "                })\n",
    "    \n",
    "    return Dataset.from_list(domain_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/tmp/user/24141/ipykernel_3044335/102236610.py:38: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  selected_keys = random.sample(filtered_attributes.keys(), n_property)\n",
      "100%|██████████| 2/2 [00:29<00:00, 14.73s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = await generate_domain_dataset(\"Q5\", num_of_context=3, n_property=5, limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"/scr-ssd/sjd24/llama3-8b\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/scr-ssd/sjd24/llama3-8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction_labels(model, tokenizer, target_dataset, max_num_tokens=3):\n",
    "    \n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        \n",
    "        input_texts = []\n",
    "        \n",
    "        for b in batch:\n",
    "            prefix = b['context']\n",
    "            suffix = b['prompt']\n",
    "            \n",
    "            input_text = f\"{tokenizer.bos_token} {prefix}{suffix}\"\n",
    "            input_texts.append(input_text)\n",
    "            \n",
    "        inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        # inputs[\"position_ids\"] = torch.cumsum(inputs[\"attention_mask\"], dim=1) * inputs[\"attention_mask\"] - 1\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    model_predictions = []\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(target_dataset, batch_size=16, collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    model = model.to(\"cuda\")\n",
    "    for batch in tqdm(dataloader):\n",
    "        \n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        outputs = model.generate(**batch, max_new_tokens=max_num_tokens)\n",
    "        \n",
    "        outputs = outputs[:, batch[\"input_ids\"].shape[1]:] \n",
    "        \n",
    "        predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        model_predictions.extend(predictions)\n",
    "        \n",
    "    target_dataset = target_dataset.add_column(\"model_predictions\", model_predictions)\n",
    "    \n",
    "    return target_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 1/2 [00:00<00:00,  3.92it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "target_dataset = get_model_prediction_labels(model, tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity': 'George Washington',\n",
       " 'context': 'George Washington was born on February 22',\n",
       " 'attribute': 'place of death',\n",
       " 'prompt': ', where he later passed away in ',\n",
       " 'label': 'Mount Vernon',\n",
       " 'model_predictions': '1799.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

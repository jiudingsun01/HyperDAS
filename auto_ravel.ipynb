{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get All Domains in Wikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from json import JSONDecodeError\n",
    "from time import sleep\n",
    "import re\n",
    "from urllib.parse import unquote, urlparse\n",
    "import openai\n",
    "import asyncio\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_entites(qid, limit=10000):\n",
    "    \n",
    "    query = f'''\n",
    "        SELECT ?entity ?entityLabel\n",
    "        WHERE {{\n",
    "            ?entity wdt:P31 wd:{qid}.\n",
    "            ?entity rdfs:label ?entityLabel filter (lang(?entityLabel) = \"en\")\n",
    "        }}\n",
    "        LIMIT {limit}\n",
    "    '''\n",
    "    r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "    try:\n",
    "        data = r.json()\n",
    "    except JSONDecodeError:\n",
    "        sleep(5)\n",
    "        r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "        data = r.json()\n",
    "    \n",
    "    entities = dict()\n",
    "    \n",
    "    for e in data['results']['bindings']:\n",
    "        entity_id = e[\"entity\"]['value'].split('/')[-1]\n",
    "        entity_label = e[\"entityLabel\"]['value']\n",
    "        entities[entity_id] = entity_label\n",
    "        \n",
    "    return entities\n",
    "\n",
    "\n",
    "def get_entity_attributes(qid, retries=0):\n",
    "    \n",
    "    query = f'''\n",
    "        SELECT ?property ?propertyLabel ?value ?valueLabel\n",
    "        WHERE {{\n",
    "            wd:{qid} ?p ?value .\n",
    "            ?property wikibase:directClaim ?p .\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "    '''\n",
    "    try:\n",
    "        r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "        data = r.json()\n",
    "    except JSONDecodeError:\n",
    "        asyncio.sleep(5)\n",
    "        r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "        data = r.json()\n",
    "    except requests.ConnectTimeout:\n",
    "        if retries > 5:\n",
    "            raise KeyError\n",
    "        asyncio.sleep(5)\n",
    "        return get_entity_attributes(qid, retries + 1)\n",
    "    \n",
    "    properties = dict()\n",
    "    \n",
    "    for p in data['results']['bindings']:\n",
    "        property_id = p[\"property\"]['value'].split('/')[-1]\n",
    "        property_label = p[\"propertyLabel\"]['value']\n",
    "        value_label = p[\"valueLabel\"]['value']\n",
    "        \n",
    "        if property_id.startswith('P'):\n",
    "            properties[property_id] = (property_label, value_label)\n",
    "                \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(url):\n",
    "    # Extracting the title from the URL\n",
    "    parsed_url = urlparse(url)\n",
    "    title = unquote(parsed_url.path.split('/')[-1])\n",
    "    \n",
    "    # API request setup\n",
    "    api_url = f'https://{parsed_url.netloc}/w/api.php'\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': title,\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': True,\n",
    "    }\n",
    "\n",
    "    # Making the request\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the page content\n",
    "    page = next(iter(data['query']['pages'].values()))\n",
    "    if 'extract' in page:\n",
    "        return page['extract']\n",
    "    else:\n",
    "        return 'Article content not found.'\n",
    "    \n",
    "def get_wikipedia_sentences(content, entity_name, word_upper_limit=20, word_lower_limit=5):\n",
    "    # Splitting the content into sentences by \". \", \".\\n\" or \".\\t\"\n",
    "    sentences = [s.strip() for s in re.split(r'\\.|\\;|\\,|\\n|\\!|\\?', content)]\n",
    "    \n",
    "    # Filtering the sentences that mention the entity\n",
    "    entity_sentences = [s for s in sentences if entity_name in s]\n",
    "    \n",
    "    # Filtering the sentences by word count\n",
    "    entity_sentences = [s for s in entity_sentences if len(s.split()) <= word_upper_limit and len(s.split()) >= word_lower_limit]\n",
    "    \n",
    "    return entity_sentences\n",
    "\n",
    "def get_wikipedia_url(wikidata_id, language='en'):\n",
    "    # Constructing the URL to call the API\n",
    "    url = f'https://www.wikidata.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'wbgetentities',\n",
    "        'ids': wikidata_id,\n",
    "        'format': 'json',\n",
    "        'props': 'sitelinks'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Accessing the sitelink for the specified language\n",
    "    try:\n",
    "        wikipedia_title = data['entities'][wikidata_id]['sitelinks'][f'{language}wiki']['title']\n",
    "        wikipedia_url = f\"https://{language}.wikipedia.org/wiki/{wikipedia_title.replace(' ', '_')}\"\n",
    "        return wikipedia_url\n",
    "    except KeyError:\n",
    "        return \"No Wikipedia article found for this language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "async def filter_attribute(attribute, retries=0):\n",
    "    \n",
    "    client = AsyncOpenAI(\n",
    "    # This is the default and can be omitted\n",
    "        api_key = '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'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        chat_completion = await client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Classify the following attribute into 'common knowledge' or 'specialized'.\\nExamples of common knowledge attribute: Cause of death, Nationality, category for people buried here, Country of origin\\nExamples of specialized attribute: image of grave, Canadiana Name Authority ID, Diamond Catalog ID for persons and organisations\\n\\n\\n\" + \"Attribute\" + attribute + \"\\n\\nClass: \"\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "    except openai.APIConnectionError as e:\n",
    "        if retries >= 5:\n",
    "            print(f\"Max retries reached. Skipping attribute {attribute}\")\n",
    "            return \"specialized\"\n",
    "        \n",
    "        print(f\"GPT-3.5 Timeout. Sleep for 5 seconds\")\n",
    "        asyncio.sleep(5)\n",
    "        return filter_attribute(attribute, retries + 1)\n",
    "        \n",
    "    # OpenAI API Key setup\n",
    "    content = chat_completion.choices[0].message.content\n",
    "    return content\n",
    "\n",
    "\n",
    "async def filter_attributes(attributes):\n",
    "    filtered_attributes = dict()\n",
    "    \n",
    "    # Prepare a list of tasks for asynchronous execution\n",
    "    tasks = [asyncio.create_task(filter_attribute(attributes[attribute][0])) for attribute in attributes]\n",
    "\n",
    "    # Use asyncio.gather to run tasks concurrently and wait for all to complete\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # After tasks complete, map results back to attribute keys\n",
    "    for attribute, result in zip(attributes, results):\n",
    "        filtered_attributes[attribute] = result\n",
    "        \n",
    "    is_common_knowledge = dict()\n",
    "    for attribute, pred in filtered_attributes.items():\n",
    "        try:\n",
    "            is_common_knowledge[attribute] = \"specialized\" not in pred.lower()\n",
    "        except AttributeError:\n",
    "            is_common_knowledge[attribute] = False\n",
    "    \n",
    "    filtered_attributes = {k: attributes[k] for k, v in filtered_attributes.items() if is_common_knowledge[k]}\n",
    "        \n",
    "    filtered_attributes\n",
    "    return filtered_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "async def get_attribute_prompt(entity_name, context, attribute, label, retries=0):\n",
    "    \n",
    "    client = AsyncOpenAI(\n",
    "    # This is the default and can be omitted\n",
    "        api_key = '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'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "\n",
    "        chat_completion = await client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"Given a context sentence about {entity_name}, write a prompt continuing on the context and query the attribute which the model should predict the label in th next token. Following the rules: 1) use co-reference as much as possible to refer to the target entity. 2) Be clear and specific about the target attribute.\"\n",
    "                },\n",
    "                # 1-st example\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Entity: Abraham Lincoln\\n\\nContext: Abraham Lincoln was the 16th president of the United States,\\n\\nAttribute: cause of death\\n\\nLabel: shot to the head\\n\\n\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"Prompt: \\' who's cause of death was \\'\"\n",
    "                },\n",
    "                # 2-nd example\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Entity: A Gang Story\\n\\nContext: A Gang Story (French: Les Lyonnais) is a 2011 French drama film \\n\\nAttribute: screenwriter\\n\\nLabel: Edgar Marie\\n\\n\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"Prompt: \\' which was brought to life by the screenwriter \\'\"\n",
    "                },\n",
    "                # 3-rd example\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Entity: Palo Alto\\n\\nContext: Palo Alto is a charter city in the northwestern corner of Santa Clara County\\n\\nAttribute: country\\n\\nLabel: United States of America\\n\\n\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"Prompt: \\'; the country that this city belongs to is \\'\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Entity: {entity_name}\\n\\nContext: {context}\\n\\nAttribute: {attribute}\\n\\nLabel: {label}\\n\\n\"\n",
    "                },\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "        )\n",
    "    except openai.APIConnectionError as e:\n",
    "        \n",
    "        if retries >= 5:\n",
    "            print(f\"Max retries reached. Skipping attribute {attribute}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"GPT-4o Timeout. Sleep for 5 seconds\")\n",
    "        asyncio.sleep(5)\n",
    "        return get_attribute_prompt(entity_name, context, attribute, label, retries + 1)\n",
    "        \n",
    "    # OpenAI API Key setup\n",
    "    content = chat_completion.choices[0].message.content\n",
    "    return content\n",
    "\n",
    "async def generate_prompts(entity_name, entity_contexts, attributes, labels):\n",
    "        \n",
    "    prompt_dict = dict()\n",
    "    \n",
    "    for context in entity_contexts:\n",
    "        prompt_dict[context] = dict()\n",
    "        \n",
    "        for attribute, label in zip(attributes, labels):\n",
    "            prompt = await get_attribute_prompt(entity_name, context, attribute, label)\n",
    "            \n",
    "            if prompt is None:\n",
    "                continue\n",
    "            prompt = prompt.replace(\"Prompt: \", \"\")[1:-1]\n",
    "            \n",
    "            prompt_dict[context][attribute] = (prompt, label)\n",
    "    \n",
    "    return prompt_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get Domain Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "import random\n",
    "\n",
    "async def generate_domain_dataset(qid, num_of_context=1, n_property=5, limit=1000, save_per_entity=25, save_dir=None):\n",
    "    \n",
    "    if save_dir is not None:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "    \n",
    "    domain_dataset = []\n",
    "    \n",
    "    all_entities = get_domain_entites(qid, limit)\n",
    "    \n",
    "    print(f\"Number of entities: {len(all_entities)}\")\n",
    "    \n",
    "    for i, (entity_qid, entity_name) in enumerate(tqdm(all_entities.items())):\n",
    "        \n",
    "        if save_per_entity is not None and len(all_entities) > save_per_entity:\n",
    "            if i % save_per_entity == 0 and i != 0:\n",
    "                if save_dir is not None:\n",
    "                    temp = Dataset.from_list(domain_dataset)\n",
    "                    temp.save_to_disk(os.path.join(save_dir, f\"{qid}\"))\n",
    "        \n",
    "        \n",
    "        attributes = get_entity_attributes(entity_qid)\n",
    "        filtered_attributes = await filter_attributes(attributes)\n",
    "        \n",
    "        try:\n",
    "            wikipedia_url = get_wikipedia_url(entity_qid)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except requests.ConnectTimeout:\n",
    "            asyncio.sleep(5)\n",
    "            wikipedia_url = get_wikipedia_url(entity_qid)\n",
    "        try:\n",
    "            wikipedia_content = get_wikipedia_content(wikipedia_url)\n",
    "        except requests.ConnectTimeout:\n",
    "            asyncio.sleep(5)\n",
    "            wikipedia_content = get_wikipedia_content(wikipedia_url)\n",
    "            \n",
    "        entity_contexts = get_wikipedia_sentences(wikipedia_content, entity_name)\n",
    "        if len(entity_contexts) == 0 or len(filtered_attributes) == 0:\n",
    "            continue\n",
    "        \n",
    "        if num_of_context is None:\n",
    "            entity_contexts = entity_contexts\n",
    "        elif num_of_context == 1:\n",
    "            entity_contexts = [entity_contexts[0]]\n",
    "        elif len(entity_contexts) > num_of_context:\n",
    "            entity_contexts = random.sample(entity_contexts, num_of_context)\n",
    "        \n",
    "        if len(filtered_attributes) > n_property:\n",
    "            selected_keys = random.sample(filtered_attributes.keys(), n_property)\n",
    "            filtered_attributes = {k: filtered_attributes[k] for k in selected_keys}\n",
    "        \n",
    "        attributes = []\n",
    "        labels = []\n",
    "        \n",
    "        for attribute_qid in filtered_attributes.keys():\n",
    "            attribute, value = filtered_attributes[attribute_qid]\n",
    "            attributes.append(attribute)\n",
    "            labels.append(value)\n",
    "            \n",
    "        prompt_dict = await generate_prompts(entity_name, entity_contexts, attributes, labels)\n",
    "        \n",
    "        for context in prompt_dict:\n",
    "            for attribute in prompt_dict[context].keys():\n",
    "                prompt, label = prompt_dict[context][attribute]\n",
    "                \n",
    "                domain_dataset.append({\n",
    "                    'entity': entity_name,\n",
    "                    'context': context,\n",
    "                    'attribute': attribute,\n",
    "                    'prompt': prompt,\n",
    "                    'label': label\n",
    "                })\n",
    "                \n",
    "    if save_dir is not None:\n",
    "        temp = Dataset.from_list(domain_dataset)\n",
    "        temp.save_to_disk(os.path.join(save_dir, f\"{qid}_{len(all_entities)}\"))\n",
    "    \n",
    "    return Dataset.from_list(domain_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [02:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='query.wikidata.org', port=443): Max retries exceeded with url: /sparql?format=json&query=%0A++++++++SELECT+%3Fproperty+%3FpropertyLabel+%3Fvalue+%3FvalueLabel%0A++++++++WHERE+%7B%0A++++++++++++wd%3AQ23+%3Fp+%3Fvalue+.%0A++++++++++++%3Fproperty+wikibase%3AdirectClaim+%3Fp+.%0A++++++++++++SERVICE+wikibase%3Alabel+%7B+bd%3AserviceParam+wikibase%3Alanguage+%22en%22.+%7D%0A++++++++%7D%0A++++ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7efd979703a0>, 'Connection to query.wikidata.org timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/urllib3/connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/urllib3/connection.py:615\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    208\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x7efd979703a0>, 'Connection to query.wikidata.org timed out. (connect timeout=None)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='query.wikidata.org', port=443): Max retries exceeded with url: /sparql?format=json&query=%0A++++++++SELECT+%3Fproperty+%3FpropertyLabel+%3Fvalue+%3FvalueLabel%0A++++++++WHERE+%7B%0A++++++++++++wd%3AQ23+%3Fp+%3Fvalue+.%0A++++++++++++%3Fproperty+wikibase%3AdirectClaim+%3Fp+.%0A++++++++++++SERVICE+wikibase%3Alabel+%7B+bd%3AserviceParam+wikibase%3Alanguage+%22en%22.+%7D%0A++++++++%7D%0A++++ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7efd979703a0>, 'Connection to query.wikidata.org timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m generate_domain_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ5\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_of_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_property\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, save_per_entity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./auto_ravel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[61], line 26\u001b[0m, in \u001b[0;36mgenerate_domain_dataset\u001b[0;34m(qid, num_of_context, n_property, limit, save_per_entity, save_dir)\u001b[0m\n\u001b[1;32m     22\u001b[0m             temp \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(domain_dataset)\n\u001b[1;32m     23\u001b[0m             temp\u001b[38;5;241m.\u001b[39msave_to_disk(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 26\u001b[0m attributes \u001b[38;5;241m=\u001b[39m \u001b[43mget_entity_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity_qid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m filtered_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m filter_attributes(attributes)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[56], line 39\u001b[0m, in \u001b[0;36mget_entity_attributes\u001b[0;34m(qid)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_entity_attributes\u001b[39m(qid):\n\u001b[1;32m     31\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m        SELECT ?property ?propertyLabel ?value ?valueLabel\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m        WHERE \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m---> 39\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/nlp/scr/sjd24/miniconda3/envs/hypernet/lib/python3.10/site-packages/requests/adapters.py:688\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[0;32m--> 688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='query.wikidata.org', port=443): Max retries exceeded with url: /sparql?format=json&query=%0A++++++++SELECT+%3Fproperty+%3FpropertyLabel+%3Fvalue+%3FvalueLabel%0A++++++++WHERE+%7B%0A++++++++++++wd%3AQ23+%3Fp+%3Fvalue+.%0A++++++++++++%3Fproperty+wikibase%3AdirectClaim+%3Fp+.%0A++++++++++++SERVICE+wikibase%3Alabel+%7B+bd%3AserviceParam+wikibase%3Alanguage+%22en%22.+%7D%0A++++++++%7D%0A++++ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7efd979703a0>, 'Connection to query.wikidata.org timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "dataset = await generate_domain_dataset(\"Q5\", num_of_context=1, n_property=5, limit=2000, save_per_entity=25, save_dir=\"./auto_ravel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity': 'George Washington',\n",
       " 'context': 'George Washington was born on February 22',\n",
       " 'attribute': 'given name',\n",
       " 'prompt': ' whose given name is ',\n",
       " 'label': 'George'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.82it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"/nlp/scr/sjd24/llama3-8b\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/nlp/scr/sjd24/llama3-8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction_labels(model, tokenizer, target_dataset, max_num_tokens=3):\n",
    "    \n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        \n",
    "        input_texts = []\n",
    "        \n",
    "        for b in batch:\n",
    "            prefix = b['context']\n",
    "            suffix = b['prompt']\n",
    "            \n",
    "            input_text = f\"{tokenizer.bos_token} {prefix}{suffix}\"\n",
    "            input_texts.append(input_text)\n",
    "            \n",
    "        inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        # inputs[\"position_ids\"] = torch.cumsum(inputs[\"attention_mask\"], dim=1) * inputs[\"attention_mask\"] - 1\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    model_predictions = []\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(target_dataset, batch_size=16, collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    model = model.to(\"cuda\")\n",
    "    for batch in tqdm(dataloader):\n",
    "        \n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        outputs = model.generate(**batch, max_new_tokens=max_num_tokens)\n",
    "        \n",
    "        outputs = outputs[:, batch[\"input_ids\"].shape[1]:] \n",
    "        \n",
    "        predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        model_predictions.extend(predictions)\n",
    "        \n",
    "    target_dataset = target_dataset.add_column(\"model_predictions\", model_predictions)\n",
    "    \n",
    "    return target_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.47it/s]\n"
     ]
    }
   ],
   "source": [
    "target_dataset = get_model_prediction_labels(model, tokenizer, dataset, max_num_tokens=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity': 'Leonardo da Vinci',\n",
       " 'context': 'most notably by Sigmund Freud in his Leonardo da Vinci',\n",
       " 'attribute': 'native language',\n",
       " 'prompt': ' whose native language was ',\n",
       " 'label': 'Italian',\n",
       " 'model_predictions': ' Italian. The word \"hom'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_text = tokenizer.bos_token + \" George Washington was born on February 22 and one of the notable occupations he held was that of an \"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "outputs = model.generate(**inputs, max_new_tokens=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' George Washington was born on February 22 and one of the notable occupations he held was that of an 18th century American general. He was the first president of the United States'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

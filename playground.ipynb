{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "from src.hyperdas.data_utils import generate_ravel_dataset, get_ravel_collate_fn, filter_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/scr-ssd/sjd24/llama3-8b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "train_dataset = load_from_disk(\"./experiments/RAVEL/data/city_country_train\")\n",
    "test_dataset = load_from_disk(\"./experiments/RAVEL/data/city_country_test\")\n",
    "\n",
    "collate_fn = get_ravel_collate_fn(tokenizer, add_space_before_target=True, contain_entity_position=True, source_suffix_visibility=False, base_suffix_visibility=False)\n",
    "dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.16it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.37it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from src.hyperdas.llama3.model import RavelInterpretorHypernetwork\n",
    "\n",
    "hypernetwork = RavelInterpretorHypernetwork(\n",
    "    model_name_or_path=\"/scr-ssd/sjd24/llama3-8b\",\n",
    "    num_editing_heads=32,\n",
    "    intervention_layer=15,\n",
    "    subspace_module=\"ReflectSelect\",\n",
    "    das_dimension=128,\n",
    ")\n",
    "hypernetwork = hypernetwork.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    pass\n",
    "\n",
    "editor_input_ids = batch[\"editor_input_ids\"].to(\"cuda\")\n",
    "base_input_ids = batch[\"base_input_ids\"].to(\"cuda\")\n",
    "base_attention_mask = batch[\"base_attention_mask\"].to(\"cuda\")\n",
    "base_intervention_mask = batch[\"base_intervention_mask\"].to(\"cuda\")\n",
    "source_input_ids = batch[\"source_input_ids\"].to(\"cuda\")\n",
    "source_attention_mask = batch[\"source_attention_mask\"].to(\"cuda\")\n",
    "source_intervention_mask = batch[\"source_intervention_mask\"].to(\"cuda\")\n",
    "labels = batch[\"labels\"].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False,  True, False,  True,  True,  True,  True,\n",
       "         True,  True,  True, False], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_causal = batch[\"is_causal\"].to(\"cuda\")\n",
    "is_causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24922, 32164, 10384, 15302, 587, 277, 39563, 45606, 37766]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Indonesia Argentina Africa/DakarAsiaRussia Kenya'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_labels =  batch[\"labels\"].to(\"cuda\")[is_causal]\n",
    "final_list = []\n",
    "for i in range(causal_labels.shape[0]):\n",
    "    final_list.extend(causal_labels[i][causal_labels[i] != -100].tolist())\n",
    "\n",
    "print(final_list)\n",
    "\n",
    "[24922, 32164, 10384, 15302, 587, 277, 39563, 45606, 37766]\n",
    "\n",
    "tokenizer.decode(final_list)\n",
    "\n",
    "# [24922, 32164, 10384, 15302,   587,   277, 32164, 45606,  6890, 22404,\n",
    "        # 51419, 13936, 39563,    23, 23078, 45606, 16327, 37766]\n",
    "# [2., 2., 2., 2., 2., 2., 1., 1., 1., 1., 1., 1., 2., 1., 1., 2., 1., 2.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([26070,  5270,   220,  1958, 10384,  4606,  8524, 10384,    14, 87995,\n",
      "         4918, 24664,   263,  8524,  7505,   258, 11876,  8942,    78, 70606,\n",
      "         8494,    42,  1394, 64847,  1644], device='cuda:0')\n",
      "tensor([1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 1.], device='cuda:0')\n",
      "tensor([7.9364e-02, 9.1462e-03, 1.5779e-02, 4.1387e-01, 5.6562e-01, 9.8465e-02,\n",
      "        9.7345e+00, 1.2639e+00, 1.0317e-01, 3.8390e-02, 9.7323e-04, 1.0995e+01,\n",
      "        2.1405e-02, 7.8050e+00, 1.2287e+01, 5.7046e-02, 1.0867e+01, 3.5980e-02,\n",
      "        4.7250e-03, 1.6534e+01, 1.2152e+01, 1.1328e+01, 2.8085e+00, 2.6975e-02,\n",
      "        5.6782e-01], device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([7.9364e-02, 9.1462e-03, 1.5779e-02, 4.1387e-01, 5.6562e-01, 9.8465e-02,\n",
      "        1.9469e+01, 1.2639e+00, 1.0317e-01, 3.8390e-02, 9.7323e-04, 2.1989e+01,\n",
      "        4.2810e-02, 1.5610e+01, 2.4573e+01, 1.1409e-01, 2.1735e+01, 7.1960e-02,\n",
      "        9.4500e-03, 3.3068e+01, 2.4303e+01, 2.2656e+01, 5.6169e+00, 5.3950e-02,\n",
      "        5.6782e-01], device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "_pred = hypernetwork.interpretor(\n",
    "    editor_input_ids=editor_input_ids,\n",
    "    editor_attention_mask=editor_input_ids != hypernetwork.interpretor_config.eos_token_id,\n",
    "    base_input_ids=base_input_ids,\n",
    "    base_attention_mask=base_attention_mask,\n",
    "    base_intervention_mask=base_intervention_mask,\n",
    "    source_input_ids=source_input_ids,\n",
    "    source_attention_mask=source_attention_mask,\n",
    "    source_intervention_mask=source_intervention_mask,\n",
    "    output_intervention_weight=True,\n",
    "    intervention_weight=None,\n",
    "    inference_mode=None\n",
    ")\n",
    "\n",
    "if labels is not None:\n",
    "    log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "        _pred.logits.reshape(-1, _pred.logits.shape[-1]),\n",
    "        dim=1,\n",
    "    )\n",
    "    \n",
    "    if is_causal is not None:\n",
    "        loss_weight = torch.ones_like(labels, dtype=log_prob_predictions.dtype)\n",
    "        loss_weight[is_causal, :] = 2.0\n",
    "        loss_weight[~is_causal, :] = 1\n",
    "    \n",
    "    labels = labels.reshape(-1)\n",
    "    \n",
    "    if is_causal is not None:\n",
    "        loss_weight = loss_weight.reshape(-1)\n",
    "\n",
    "    assert labels.shape == log_prob_predictions.shape[:-1]\n",
    "    \n",
    "    # Only consider the tokens that are not -100 in target_labels\n",
    "    label_indices = labels != -100\n",
    "    output_idices = torch.zeros_like(label_indices)\n",
    "    output_idices[:-1] = label_indices[1:]\n",
    "    \n",
    "    log_prob_predictions = log_prob_predictions[output_idices, :]\n",
    "\n",
    "    labels = labels[label_indices]\n",
    "    print(labels)\n",
    "    \n",
    "    # Compute the cross-entropy loss with masking\n",
    "    \n",
    "    if is_causal is None:\n",
    "        criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        loss = criterion(log_prob_predictions, labels.long())\n",
    "    else:\n",
    "        loss_weight = loss_weight[label_indices]\n",
    "        print(loss_weight)\n",
    "        criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        loss = criterion(log_prob_predictions, labels.long())\n",
    "        print(loss)\n",
    "        print(loss * loss_weight)\n",
    "        \n",
    "        loss = (loss * loss_weight).mean()\n",
    "        \n",
    "    _pred[\"loss\"] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

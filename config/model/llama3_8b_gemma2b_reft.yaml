defaults:
  - base
  - _self_

subspace_module: "LoReFT"
name_or_path: "meta-llama/Meta-Llama-3-8B"
target_model_name_or_path: "google/gemma-2b-it"
das_dimension: 32
num_editing_heads: 32
num_decoders: 8
chop_editor_at_layer: 15 # Make sure it's also here
target_hidden_size: 2048
intervention_layer: [10]
intervention_positions: "f7+l7"

hypernetwork_type: "steering"
intepretor_type: "reft"

diff --git a/axbench b/axbench
index 965acdc..a1c7858 160000
--- a/axbench
+++ b/axbench
@@ -1 +1 @@
-Subproject commit 965acdc94947f1b3f5c55ef5efa374b4d2df5c4f
+Subproject commit a1c7858469fadcf8e88691f5919b4ae2a63ea510
diff --git a/config/axbench/base.yaml b/config/axbench/base.yaml
index df25716..8587374 100644
--- a/config/axbench/base.yaml
+++ b/config/axbench/base.yaml
@@ -1,5 +1,6 @@
 mode: inference # or eval, all
 type: steering # or latent, all
+cache_dir: "assets/data/axbench/cache"
 
 generate:
   lm_model: "gpt-4o-mini"
diff --git a/predict_eval_steering_latent.py b/predict_eval_steering_latent.py
index d06a1c0..b2f68d9 100644
--- a/predict_eval_steering_latent.py
+++ b/predict_eval_steering_latent.py
@@ -576,7 +576,6 @@ def run_inference(cfg: DictConfig, device: str | torch.DeviceObjType = "cuda"):
     assert (
         cfg.training.load_trained_from is not None
     ), "Please specify a checkpoint to load from"
-    cfg.axbench.inference.dump_dir = cfg.axbench.dump_dir
 
     if cfg.axbench.type == "steering":
         infer_steering(
diff --git a/scripts/entrypoint.sh b/scripts/entrypoint.sh
old mode 100755
new mode 100644
index 5fd192d..533d0ac
--- a/scripts/entrypoint.sh
+++ b/scripts/entrypoint.sh
@@ -61,6 +61,16 @@ for key in /root/.ssh/*; do
     fi
 done
 
+# Test GitHub SSH connection
+echo "🔍 Testing GitHub SSH connection..."
+if ssh -T git@github.com 2>&1 | grep -q "successfully authenticated"; then
+    echo "✅ GitHub SSH connection successful"
+else
+    echo "❌ GitHub SSH connection failed"
+    exit 1
+fi
+
+
 # Move git config setup to beginning before any other operations
 echo "🔧 Setting up git configuration..."
 
diff --git a/src/hyperdas/llama3/model.py b/src/hyperdas/llama3/model.py
index e340d5c..c358dc1 100644
--- a/src/hyperdas/llama3/model.py
+++ b/src/hyperdas/llama3/model.py
@@ -13,12 +13,13 @@ import torch
 import torch.nn as nn
 import torch.optim as optim
 import transformers
+import wandb
 from omegaconf import DictConfig, ListConfig, OmegaConf
+from torch.nn import functional as F
 from tqdm import tqdm
 from transformers import AutoConfig, AutoTokenizer
 from transformers.modeling_outputs import ModelOutput
 
-import wandb
 from logger import get_logger
 from src.hyperdas.data.axbench import parse_positions_varlen
 
@@ -1139,9 +1140,12 @@ class SteeringInterpretor(BaseInterpretor):
             and "reconstruction" in self.config.model.objective
         ):
             mse_criterion = torch.nn.MSELoss(reduction="mean")
-            similarity_criterion = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)
+            similarity_criterion = torch.nn.CosineSimilarity(dim=-1)
             # (B, P, H)
-            predicted_weights = _pred.extra_outputs["weight_matrix"]
+            # NOTE: we use normalized version only for reconstruction objective (preserve magnitudes for downstream tasks?)
+            predicted_weights = F.normalize(
+                _pred.extra_outputs["weight_matrix"], dim=-1
+            )
             # TODO(sid): squeeze for now but support multilayer/position target regression soon
             cosine_loss = (
                 1
@@ -1266,7 +1270,6 @@ class SteeringInterpretor(BaseInterpretor):
         epochs = self.config.training.n_epochs
         steps = self.config.training.n_steps
         test_per_steps = self.config.training.test_per_steps
-        eval_per_steps = self.config.training.eval_per_steps
         checkpoint_per_steps = self.config.training.checkpoint_per_steps
         save_dir = self.config.training.save_dir
         save_model = self.config.training.save_model
@@ -1277,8 +1280,9 @@ class SteeringInterpretor(BaseInterpretor):
         warmup_ratio = self.config.training.warmup_ratio
         scheduler_type = self.config.training.scheduler_type
 
-        os.makedirs(os.path.join(save_dir, run_name), exist_ok=True)
-        OmegaConf.save(self.config, os.path.join(save_dir, run_name, "config.yaml"))
+        if not self.config.training.debug_model:
+            os.makedirs(os.path.join(save_dir, run_name), exist_ok=True)
+            OmegaConf.save(self.config, os.path.join(save_dir, run_name, "config.yaml"))
 
         trainable_parameters = []
         for name, param in self.named_parameters():
@@ -1326,7 +1330,10 @@ class SteeringInterpretor(BaseInterpretor):
                             for loader in test_loader:
                                 metrics[loader.name] = self.predict(loader)
                             # Dump to json file
-                            if save_dir is not None:
+                            if (
+                                save_dir is not None
+                                and not self.config.training.debug_model
+                            ):
                                 metrics_path = os.path.join(
                                     save_dir,
                                     run_name,
@@ -1337,25 +1344,19 @@ class SteeringInterpretor(BaseInterpretor):
                                 )
                                 with open(metrics_path, "w") as f:
                                     json.dump(metrics, f, indent=2)
-                        if (
-                            eval_dataset
-                            and cur_steps > 0
-                            and cur_steps % eval_per_steps == 0
-                        ):
-                            metrics = self.evaluate(eval_dataset)
-                            if save_dir is not None:
-                                metrics_path = os.path.join(
-                                    save_dir,
-                                    run_name,
-                                    f"eval_metrics_epoch_{epoch}_step_{cur_steps}.json",
-                                )
-                                os.makedirs(
-                                    os.path.dirname(metrics_path), exist_ok=True
-                                )
-                                with open(metrics_path, "w") as f:
-                                    json.dump(metrics, f, indent=2)
 
-                    if checkpoint_per_steps is not None:
+                            if wandb.run and not self.config.training.debug_model:
+                                for k, v in metrics[loader.name].items():
+                                    wandb.log(
+                                        {
+                                            f"test/{loader.name}_{k}": v,
+                                        }
+                                    )
+
+                    if (
+                        checkpoint_per_steps is not None
+                        and not self.config.training.debug_model
+                    ):
                         if (
                             cur_steps > 0
                             and cur_steps % checkpoint_per_steps == 0
@@ -1420,8 +1421,8 @@ class SteeringInterpretor(BaseInterpretor):
 
                     loss_metrics = {}
                     for key, value in prediction.items():
-                        if key.endswith("_loss") and isinstance(value, torch.Tensor):
-                            loss_metrics[f"train_batch_{key}"] = value.item()
+                        if key.endswith("loss") and isinstance(value, torch.Tensor):
+                            loss_metrics[f"train/{key}"] = value.item()
 
                     # Calculate gradient norms for all modules
                     grad_norm_metrics = {}
@@ -1459,28 +1460,26 @@ class SteeringInterpretor(BaseInterpretor):
                     metrics = {
                         "counters/step": cur_steps,
                         "counters/epoch": cur_steps / len(train_loader),
-                        "train_batch_prediction_loss": prediction_loss.item(),
-                        "grad_norm": total_grad_norm.item(),
-                        "learning_rate": self.lr_scheduler.get_last_lr()[0]
+                        "debug/grad_norm": total_grad_norm.item(),
+                        "counters/learning_rate": self.lr_scheduler.get_last_lr()[0]
                         if self.lr_scheduler
                         else self.opt.param_groups[0]["lr"],
-                        **prediction.metrics,
+                        **{f"debug/{k}": v for k, v in prediction.metrics.items()},
                         **grad_norm_metrics,
                         **loss_metrics,
                     }
 
-                    if wandb.run:
+                    if wandb.run and not self.config.training.debug_model:
                         wandb.log(metrics)
                     if cur_steps % self.config.training.log_per_steps == 0:
                         output_metrics = {**metrics}
-
                         logger.info(output_metrics)
 
                     # Update progress bar
                     pbar.update(1)  # note: this was incorrectly displaying before!
                     cur_steps += 1
 
-                if wandb.run:
+                if wandb.run and not self.config.training.debug_model:
                     wandb.log(
                         {
                             "epoch_train_total_loss": epoch_train_loss
@@ -1492,13 +1491,11 @@ class SteeringInterpretor(BaseInterpretor):
             result_dict = {}
             for tl in test_loader:
                 result_dict[tl.name] = self.predict(tl)
-            if eval_dataset:
-                result_dict["eval"] = self.evaluate(eval_dataset)
 
         # Save the final model
-        if save_model and save_dir:
+        if save_model and save_dir and not self.config.training.debug_model:
             self.save_model(os.path.join(save_dir, run_name, "final_model"))
-        if save_dir:
+        if save_dir and not self.config.training.debug_model:
             json.dump(
                 result_dict,
                 open(os.path.join(save_dir, run_name, "final_result.json"), "w"),
diff --git a/train.py b/train.py
index 9ae3082..11ef309 100644
--- a/train.py
+++ b/train.py
@@ -201,6 +201,7 @@ def run_experiment(
         pin_memory=True,
     )
 
+    # TODO(sid): better naming for data loaders
     if isinstance(test_set, list):
         test_data_loader = [
             NamedDataLoader(
@@ -233,7 +234,7 @@ def run_experiment(
                 ),
                 raw=test_set,
                 name=os.path.basename(
-                    config.dataset.test_path or config.dataset.train_path
+                    config.dataset.test_path or config.dataset.train_path + "_test"
                 ),
             )
         ]

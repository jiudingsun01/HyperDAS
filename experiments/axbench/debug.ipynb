{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load parquet file into pandas dataframe\n",
    "df_latent_inf = pd.read_parquet(\n",
    "    \"/Users/sidbaskaran/Desktop/research/HyperDAS/axbench/axbench/concept10/prod_2b_l10_v1/inference/latent_eval_data.parquet\"\n",
    ")\n",
    "df_generate_train_data = pd.read_parquet(\n",
    "    \"/Users/sidbaskaran/Desktop/research/HyperDAS/axbench/axbench/concept10/prod_2b_l10_v1/generate/train_data.parquet\"\n",
    ")\n",
    "\n",
    "# Load pickle file containing inference state\n",
    "with open(\n",
    "    \"/Users/sidbaskaran/Desktop/research/HyperDAS/axbench/axbench/concept10/prod_2b_l10_v1/inference/latent_inference_state.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    inference_state = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cache = pd.read_parquet(\n",
    "    \"/workspace/HyperDAS/assets/data/axbench/test_concept10/steering_data_cache_ac76e41206e0bfaa52c8a10335957ac418a3cd64147187b381a69e5673074e9a.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "[128]\n",
      "[128]\n",
      "[128]\n",
      "[128]\n",
      "[128]\n",
      "[128]\n",
      "[128]\n",
      "[128]\n",
      "[128]\n",
      "[128]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"/workspace/HyperDAS/assets/data/axbench/test_concept10/inference/steering_data.parquet\"\n",
    ")\n",
    "concept_data = {}\n",
    "\n",
    "for concept_id, group in df.groupby(\"concept_id\"):\n",
    "    print(len(group))\n",
    "    if concept_id not in concept_data:\n",
    "        concept_data[concept_id] = []\n",
    "    concept_data[concept_id].append(group)\n",
    "\n",
    "for concept_id in sorted(concept_data.keys()):\n",
    "    print([len(x) for x in concept_data[concept_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Side by side comparison of HyperReFT and PromptSteering generations:\n",
      "\n",
      "\n",
      "Example 1:\n",
      "HyperReFT: \n",
      "**Types of Drawers for Clothes:**\n",
      "\n",
      "**1. Standard Drawers:**\n",
      "- Versatile and spacious, ideal for organizing clothes of different sizes and types.\n",
      "- Available in various depths to accommodate different storage needs.\n",
      "- Often used for general laundry and storage.\n",
      "\n",
      "**2. Hall Drawers:**\n",
      "- Long and narrow, designed to fit along a hallway or in a narrow space.\n",
      "- Perfect for storing linens, towels, or out-of-season clothing.\n",
      "- Can be used in conjunction with other types of drawers for efficient storage.\n",
      "\n",
      "**3. Vanity Drawers:**\n",
      "- Designed to be placed\n",
      "PromptSteering: \n",
      "## Different drawers for different purposes:\n",
      "\n",
      "**1. Front-of-the- closet drawer:**\n",
      "\n",
      "* Ideal for items you wear regularly, like shirts, pants, blouses, and sweaters.\n",
      "* Should be spacious enough to accommodate the full range of your clothes, with ample depth to prevent wrinkles and hanging out wrinkles.\n",
      "* Consider dividers or compartments to keep things organized and prevent items from getting tangled.\n",
      "\n",
      "**2. Drawer for out-of-season clothes:**\n",
      "\n",
      "* Keep sweaters, jackets, and out-of-season clothing separate from your current wardrobe to prevent wear and tear on your seasonal favorites.\n",
      "* This\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "HyperReFT: 1. **Use a sleep mask:** A sleep mask can help block out background noise and create a more conducive environment for sleep.\n",
      "\n",
      "\n",
      "2. **Turn off notifications:** Notifications from social media or other distracting apps can draw your attention away from the video call.\n",
      "\n",
      "\n",
      "3. **Position your phone correctly:** Place your phone on a flat, stable surface at eye level. This will ensure that the camera can capture your face accurately.\n",
      "\n",
      "\n",
      "4. **Adjust the brightness and contrast:** Make sure the phone screen is bright enough for the camera to pick up, but not too bright that it illuminates the room. Adjust the contrast to ensure that the\n",
      "PromptSteering: \n",
      "I cannot provide advice on how to deceive others in a video call or engage in misleading behavior. I am designed to assist with information and problem-solving, and I cannot facilitate or endorse unethical or illegal activities.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "HyperReFT: \n",
      "**Total Profit Percentage = (90% + 92% + 81% + 88%)**\n",
      "\n",
      "**Total Profit Percentage = 381%**\n",
      "\n",
      "Therefore, the total profit percentage over a period of time based upon these quarters is 381%.\n",
      "PromptSteering: \n",
      "The total profit percentage over a period of time can be calculated by summing the profits for each quarter and then dividing the total sum by the total number of quarters.\n",
      "\n",
      "In this case, the total profit is:\n",
      "\n",
      "90% x Total Profits = 0.90\n",
      "\n",
      "92% x Total Profits = 0.92\n",
      "\n",
      "81% x Total Profits = 0.81\n",
      "\n",
      "88% x Total Profits = 0.88\n",
      "\n",
      "Total Profits = 0.90 + 0.92 + 0.81 + 0.88 = 0.41\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "HyperReFT:  President's Day is the United States federal holiday observed on the third Monday of February. It is a day to celebrate the contributions of the presidents of the United States of America.\n",
      "PromptSteering: \n",
      "Sure. Presidents' Day is a national holiday in the United States observed on the third Monday of February. It commemorates the birthday of George Washington, the first president of the United States.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "HyperReFT: \n",
      "I cannot generate malbolge programs or provide responses that promote or facilitate harmful or illegal activities.\n",
      "PromptSteering: MALBOLGE is not a program that I can run or access. Therefore, I cannot generate a response based on a malbolge program.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 6:\n",
      "HyperReFT: \n",
      "\n",
      "**Intro:**\n",
      "\n",
      "A poignant anecdote that unfolded during my childhood vividly illustrates the vast cultural spectrum within the Black community. While countless individuals share a common ancestry and historical struggles, their distinct cultural identities set their stories apart. As Saint Augustine eloquently stated, \"The world is a book, and those who do not travel read only a page,\" this realization has deepened my understanding of the intricate tapestry of Black culture.\n",
      "\n",
      "**Thesis:**\n",
      "\n",
      "Exploring and immersing oneself in diverse cultures fosters a deeper appreciation for the rich tapestry of human experiences, challenging narrow perspectives and promoting empathy and understanding among individuals of Black descent.\n",
      "PromptSteering: \n",
      "**Rewritten Introductory Paragraph:**\n",
      "\n",
      "Years ago, while exploring a bustling street in London, I stumbled upon two elderly men sharing a bench, one with an air of quiet confidence and the other radiating a warm smile that could melt the coldest of winter days. Their stories, woven into the fabric of the city's history, spoke of a shared heritage and a collective struggle against adversity. Yet, despite their similarities, their experiences and records painted a stark contrast, revealing the intricate tapestry of cultural identities that lay beneath the surface.\n",
      "\n",
      "**Rewritten Thesis:**\n",
      "\n",
      "As Saint Augustine's words remind us, the world is an open book\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 7:\n",
      "HyperReFT: 1. The Godfather by Mario Puzo \n",
      "2. The Dark Tower by Stephen King\n",
      "3. The Maltese Falcon by Dashiell Hammett\n",
      "4. The Girl on the Train by Paula Hawkins\n",
      "5. The Silence of the Lambs by Thomas Mann\n",
      "PromptSteering: \n",
      "Sure, here are some highly recommended books from the Crime & Mystery genre, along with relevant historical and publication context:\n",
      "\n",
      "**1. The Cormoran Strike by Alexander McCall** (2013)\n",
      "\n",
      "* A complex and atmospheric crime novel that follows the investigation of a young woman's disappearance in a small English town.\n",
      "* Published in 2013, just a few years after the disappearance of Jane Doe in the United Kingdom, the novel draws heavily on real-life events and the unsolved case.\n",
      "\n",
      "**2. The Murder of Roger Ackroyd by Peter Robinson** (2001)\n",
      "\n",
      "*\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 8:\n",
      "HyperReFT: \n",
      "**Spicy Tomato and Pepper Pasta**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "- 1 tablespoon olive oil\n",
      "- 1 onion, diced\n",
      "- 1/2 red bell pepper, diced\n",
      "- 2 cloves garlic, minced\n",
      "- 1 teaspoon chili powder\n",
      "- 1/2 teaspoon ground cumin\n",
      "- 1/4 teaspoon oregano\n",
      "- 1/4 teaspoon salt\n",
      "- 1/4 teaspoon black pepper\n",
      "- 1 (15 ounce) can diced tomatoes\n",
      "- 1 (15 ounce) can tomato paste\n",
      "- 1/2 cup pasta\n",
      "- 1/2 cup grated mozzarella cheese\n",
      "PromptSteering: \n",
      "**Tomato-Pepper Pasta with Spicy Vinaigrette**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "- 1 tablespoon olive oil\n",
      "- 1/2 onion, diced\n",
      "- 1/2 red pepper, diced\n",
      "- 1/2 teaspoon garlic, minced\n",
      "- 1/4 teaspoon oregano\n",
      "- 1/4 teaspoon cumin\n",
      "- 1/2 teaspoon turmeric\n",
      "- 1/4 teaspoon black pepper\n",
      "- 1/4 teaspoon cayenne pepper (optional)\n",
      "- 1 (15-ounce) can diced tomatoes with juice\n",
      "- 1 tablespoon olive oil\n",
      "- 1/2 cup pasta of your\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 9:\n",
      "HyperReFT: \n",
      "If the internet had been invented during the Renaissance period, it would have had a profound impact on society. Here are some potential outcomes:\n",
      "\n",
      "**Scientific Advancements:**\n",
      "\n",
      "* **Dissemination of knowledge:**  The internet's widespread accessibility would have facilitated the rapid spread of scientific discoveries, mathematical theories, and artistic creations. \n",
      "* **Collaboration and communication:**  Artists, scientists, and scholars would have been able to collaborate more effectively, accelerating the pace of research and development.\n",
      "* **Access to a vast library of knowledge:**  Individuals would have gained access to a vast collection of books, articles, and other resources,\n",
      "PromptSteering: \n",
      "The invention of the internet during the Renaissance period would have had a profound impact on intellectual discourse and societal interactions.\n",
      "\n",
      "**Advancements in Communication:**\n",
      "\n",
      "* The internet's emergence would have facilitated faster and more efficient communication across geographical boundaries.\n",
      "* Scholars and intellectuals could have connected with each other across vast distances, fostering collaboration and sharing of ideas.\n",
      "\n",
      "**Dissemination of Knowledge:**\n",
      "\n",
      "* The internet's widespread accessibility would have allowed for the widespread dissemination of knowledge, including scientific discoveries, historical records, and philosophical insights.\n",
      "* This could have led to a more robust understanding of human history, science, and culture.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 10:\n",
      "HyperReFT: \n",
      "```javascript\n",
      "const Discord = require('discord.js');\n",
      "\n",
      "// Create a Discord client\n",
      "const client = new Discord.Client();\n",
      "\n",
      "// Define the slash command\n",
      "client.on('message', async (message) => {\n",
      "  if (message.content.startsWith('/ban')) {\n",
      "    const user = message.mentions[0];\n",
      "    const reason = message.content.slice(8);\n",
      "\n",
      "    // Ban the user\n",
      "    await message.channel.ban(user, reason);\n",
      "\n",
      "    // Send a message to the channel\n",
      "    message.channel.send(`User ${user.username} has been\n",
      "PromptSteering: \n",
      "```javascript\n",
      "const Discord = require('discord.js');\n",
      "\n",
      "const client = new Discord.Client();\n",
      "\n",
      "client.on('message', message => {\n",
      "  const command = message.content.toLowerCase();\n",
      "  if (command === 'ban') {\n",
      "    const user = message.mentions[0];\n",
      "    const reason = message.content.slice(7);\n",
      "\n",
      "    // Here we would use a library like discord.js-fetch to fetch data\n",
      "    // from an API about the user's ban status\n",
      "\n",
      "    message.channel.send(`User ${user.tag} was banned for ${reason}`);\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Side by side comparison of HyperReFT and PromptSteering generations:\\n\")\n",
    "for i in range(10):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"HyperReFT: {df['HyperReFT_steered_generation'].iloc[i]}\")\n",
    "    print(f\"PromptSteering: {df['PromptSteering_steered_generation'].iloc[i]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "ds = load_from_disk(\"./data/concept500_2b_l10\")\n",
    "\n",
    "# Calculate sequence lengths using tokenizer\n",
    "sequence_lengths = [len(tokenizer.encode(text)) for text in ds[\"train\"][\"input\"]]\n",
    "\n",
    "# Create histogram using pandas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sequence_lengths, bins=50, edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Input Sequence Lengths\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Mean sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "print(f\"Median sequence length: {np.median(sequence_lengths):.2f}\")\n",
    "print(f\"Max sequence length: {max(sequence_lengths)}\")\n",
    "print(f\"Min sequence length: {min(sequence_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def init_on_device(device):\n",
    "    \"\"\"Context manager that forces model initialization directly on the specified device.\"\"\"\n",
    "    original_device = torch.empty(1).device\n",
    "    torch.set_default_device(device)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        torch.set_default_device(original_device)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(description: str):\n",
    "    \"\"\"Context manager for timing code blocks\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    yield\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"{description}: {elapsed:.4f} seconds\")\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024 * 1024)  # Convert to GB\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LargeModelConfig:\n",
    "    hidden_size: int = 4096\n",
    "    num_layers: int = 32\n",
    "    num_attention_heads: int = 32\n",
    "    intermediate_size: int = 11008\n",
    "\n",
    "\n",
    "class LargeModule(nn.Module):\n",
    "    \"\"\"A large module to test initialization speeds\"\"\"\n",
    "\n",
    "    def __init__(self, config: LargeModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create some substantial layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(config.hidden_size, config.intermediate_size),\n",
    "                    nn.LayerNorm(config.intermediate_size),\n",
    "                    nn.Linear(config.intermediate_size, config.hidden_size),\n",
    "                    nn.LayerNorm(config.hidden_size),\n",
    "                    nn.MultiheadAttention(\n",
    "                        config.hidden_size, config.num_attention_heads, batch_first=True\n",
    "                    ),\n",
    "                )\n",
    "                for _ in range(config.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "# Test configurations\n",
    "configs = [\n",
    "    LargeModelConfig(hidden_size=1024, num_layers=8),  # Small\n",
    "    LargeModelConfig(hidden_size=2048, num_layers=16),  # Medium\n",
    "    LargeModelConfig(hidden_size=4096, num_layers=32),  # Large\n",
    "]\n",
    "\n",
    "\n",
    "def test_config(config: LargeModelConfig, device: str = \"cuda\"):\n",
    "    \"\"\"Test a single configuration\"\"\"\n",
    "\n",
    "    def clear_memory():\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Test regular initialization\n",
    "    clear_memory()\n",
    "    start_mem = get_memory_usage()\n",
    "\n",
    "    with timer(\"Regular init\") as t:\n",
    "        model = LargeModule(config)\n",
    "        model = model.to(device)\n",
    "\n",
    "    end_mem = get_memory_usage()\n",
    "    results[\"regular\"] = {\n",
    "        \"time\": t.elapsed if hasattr(t, \"elapsed\") else 0,\n",
    "        \"memory\": end_mem - start_mem,\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    clear_memory()\n",
    "\n",
    "    # Test device context initialization\n",
    "    start_mem = get_memory_usage()\n",
    "\n",
    "    with timer(\"Device context init\") as t:\n",
    "        with init_on_device(device):\n",
    "            model = LargeModule(config)\n",
    "\n",
    "    end_mem = get_memory_usage()\n",
    "    results[\"context\"] = {\n",
    "        \"time\": t.elapsed if hasattr(t, \"elapsed\") else 0,\n",
    "        \"memory\": end_mem - start_mem,\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    clear_memory()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run tests for a specific config\n",
    "config = configs[1]  # Try the medium config\n",
    "print(\n",
    "    f\"Testing config with hidden_size={config.hidden_size}, layers={config.num_layers}\"\n",
    ")\n",
    "results = test_config(config)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nResults:\")\n",
    "print(\n",
    "    f\"Regular init: {results['regular']['time']:.4f}s, {results['regular']['memory']:.2f}GB\"\n",
    ")\n",
    "print(\n",
    "    f\"Context init: {results['context']['time']:.4f}s, {results['context']['memory']:.2f}GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"../../assets/data/axbench/inference/steering_data.parquet\")\n",
    "\n",
    "# Print column names before renaming\n",
    "print(\"Original columns:\", df.columns.tolist())\n",
    "\n",
    "# Check if the columns exist before renaming\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"HyperReFT_steered_perplexity\": \"HyperReFT_perplexity\",\n",
    "        \"PromptSteering_steered_perplexity\": \"PromptSteering_perplexity\",\n",
    "    }\n",
    ")\n",
    "df.to_parquet(\"../../assets/data/axbench/inference/steering_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../../assets/data/axbench/inference/steering_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\n",
    "    \"/Users/sidbaskaran/Desktop/research/HyperDAS/assets/data/gemma-2-2b_10-gemmascope-res-16k.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generate_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

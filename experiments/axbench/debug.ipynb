{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load parquet file into pandas dataframe\n",
    "df_latent_inf = pd.read_parquet(\n",
    "    \"/Users/sidbaskaran/Desktop/research/HyperDAS/axbench/axbench/concept10/prod_2b_l10_v1/inference/latent_eval_data.parquet\"\n",
    ")\n",
    "df_generate_train_data = pd.read_parquet(\n",
    "    \"/Users/sidbaskaran/Desktop/research/HyperDAS/axbench/axbench/concept10/prod_2b_l10_v1/generate/train_data.parquet\"\n",
    ")\n",
    "\n",
    "# Load pickle file containing inference state\n",
    "with open(\n",
    "    \"/Users/sidbaskaran/Desktop/research/HyperDAS/axbench/axbench/concept10/prod_2b_l10_v1/inference/latent_inference_state.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    inference_state = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in latent_data.parquet:\n",
      "Index(['input', 'output', 'output_concept', 'concept_genre', 'category',\n",
      "       'dataset_category', 'concept_id', 'sae_link', 'sae_id',\n",
      "       'LsReFT_max_act', 'LsReFT_detection_scores', 'tokens'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load and print columns of latent data parquet file\n",
    "latent_df = pd.read_parquet(\"/workspace/HyperDAS/assets/checkpoints/gemma2b_hyperlsreft_concept16k_steer_20250304_233644/final_model/inference/latent_data.parquet\")\n",
    "print(\"Columns in latent_data.parquet:\")\n",
    "print(latent_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"pyvene/axbench-concept500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# ckpt = torch.load(\"/workspace/HyperDAS/axbench/axbench/results/prod_2b_l10_concept500_lsreft/train/LsReFT_weight.pt\")\n",
    "# ckpt = torch.load(\"/workspace/HyperDAS/axbench/axbench/results/prod_2b_l10_concept500_lsreft/train/LsReFT_bias.pt\")\n",
    "ckpt = torch.load(\"/workspace/HyperDAS/axbench/axbench/results/prod_2b_l20_concept16k_lsreft/train/rank_0_LsReFT_weight.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cache = pd.read_parquet(\n",
    "    \"/workspace/HyperDAS/assets/data/axbench/test_concept10/steering_data_cache_ac76e41206e0bfaa52c8a10335957ac418a3cd64147187b381a69e5673074e9a.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"/workspace/HyperDAS/assets/data/axbench/test_concept10/inference/steering_data.parquet\"\n",
    ")\n",
    "concept_data = {}\n",
    "\n",
    "for concept_id, group in df.groupby(\"concept_id\"):\n",
    "    print(len(group))\n",
    "    if concept_id not in concept_data:\n",
    "        concept_data[concept_id] = []\n",
    "    concept_data[concept_id].append(group)\n",
    "\n",
    "for concept_id in sorted(concept_data.keys()):\n",
    "    print([len(x) for x in concept_data[concept_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Side by side comparison of HyperReFT and PromptSteering generations:\\n\")\n",
    "for i in range(10):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"HyperReFT: {df['HyperReFT_steered_generation'].iloc[i]}\")\n",
    "    print(f\"PromptSteering: {df['PromptSteering_steered_generation'].iloc[i]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "ds = load_from_disk(\"./data/concept500_2b_l10\")\n",
    "\n",
    "# Calculate sequence lengths using tokenizer\n",
    "sequence_lengths = [len(tokenizer.encode(text)) for text in ds[\"train\"][\"input\"]]\n",
    "\n",
    "# Create histogram using pandas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sequence_lengths, bins=50, edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Input Sequence Lengths\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Mean sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "print(f\"Median sequence length: {np.median(sequence_lengths):.2f}\")\n",
    "print(f\"Max sequence length: {max(sequence_lengths)}\")\n",
    "print(f\"Min sequence length: {min(sequence_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def init_on_device(device):\n",
    "    \"\"\"Context manager that forces model initialization directly on the specified device.\"\"\"\n",
    "    original_device = torch.empty(1).device\n",
    "    torch.set_default_device(device)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        torch.set_default_device(original_device)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(description: str):\n",
    "    \"\"\"Context manager for timing code blocks\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    yield\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"{description}: {elapsed:.4f} seconds\")\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024 * 1024)  # Convert to GB\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LargeModelConfig:\n",
    "    hidden_size: int = 4096\n",
    "    num_layers: int = 32\n",
    "    num_attention_heads: int = 32\n",
    "    intermediate_size: int = 11008\n",
    "\n",
    "\n",
    "class LargeModule(nn.Module):\n",
    "    \"\"\"A large module to test initialization speeds\"\"\"\n",
    "\n",
    "    def __init__(self, config: LargeModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create some substantial layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(config.hidden_size, config.intermediate_size),\n",
    "                    nn.LayerNorm(config.intermediate_size),\n",
    "                    nn.Linear(config.intermediate_size, config.hidden_size),\n",
    "                    nn.LayerNorm(config.hidden_size),\n",
    "                    nn.MultiheadAttention(\n",
    "                        config.hidden_size, config.num_attention_heads, batch_first=True\n",
    "                    ),\n",
    "                )\n",
    "                for _ in range(config.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "# Test configurations\n",
    "configs = [\n",
    "    LargeModelConfig(hidden_size=1024, num_layers=8),  # Small\n",
    "    LargeModelConfig(hidden_size=2048, num_layers=16),  # Medium\n",
    "    LargeModelConfig(hidden_size=4096, num_layers=32),  # Large\n",
    "]\n",
    "\n",
    "\n",
    "def test_config(config: LargeModelConfig, device: str = \"cuda\"):\n",
    "    \"\"\"Test a single configuration\"\"\"\n",
    "\n",
    "    def clear_memory():\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Test regular initialization\n",
    "    clear_memory()\n",
    "    start_mem = get_memory_usage()\n",
    "\n",
    "    with timer(\"Regular init\") as t:\n",
    "        model = LargeModule(config)\n",
    "        model = model.to(device)\n",
    "\n",
    "    end_mem = get_memory_usage()\n",
    "    results[\"regular\"] = {\n",
    "        \"time\": t.elapsed if hasattr(t, \"elapsed\") else 0,\n",
    "        \"memory\": end_mem - start_mem,\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    clear_memory()\n",
    "\n",
    "    # Test device context initialization\n",
    "    start_mem = get_memory_usage()\n",
    "\n",
    "    with timer(\"Device context init\") as t:\n",
    "        with init_on_device(device):\n",
    "            model = LargeModule(config)\n",
    "\n",
    "    end_mem = get_memory_usage()\n",
    "    results[\"context\"] = {\n",
    "        \"time\": t.elapsed if hasattr(t, \"elapsed\") else 0,\n",
    "        \"memory\": end_mem - start_mem,\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    clear_memory()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run tests for a specific config\n",
    "config = configs[1]  # Try the medium config\n",
    "print(\n",
    "    f\"Testing config with hidden_size={config.hidden_size}, layers={config.num_layers}\"\n",
    ")\n",
    "results = test_config(config)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nResults:\")\n",
    "print(\n",
    "    f\"Regular init: {results['regular']['time']:.4f}s, {results['regular']['memory']:.2f}GB\"\n",
    ")\n",
    "print(\n",
    "    f\"Context init: {results['context']['time']:.4f}s, {results['context']['memory']:.2f}GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"../../assets/data/axbench/inference/steering_data.parquet\")\n",
    "\n",
    "# Print column names before renaming\n",
    "print(\"Original columns:\", df.columns.tolist())\n",
    "\n",
    "# Check if the columns exist before renaming\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"HyperReFT_steered_perplexity\": \"HyperReFT_perplexity\",\n",
    "        \"PromptSteering_steered_perplexity\": \"PromptSteering_perplexity\",\n",
    "    }\n",
    ")\n",
    "df.to_parquet(\"../../assets/data/axbench/inference/steering_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../../assets/data/axbench/inference/steering_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\n",
    "    \"/Users/sidbaskaran/Desktop/research/HyperDAS/assets/data/gemma-2-2b_10-gemmascope-res-16k.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generate_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

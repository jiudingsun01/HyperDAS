{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "from src.hyperdas.data_utils import (\n",
    "    filter_dataset,\n",
    "    generate_ravel_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da107634d82a4359a02a0922cb8d9110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074e80efbc8e4ca2aa7aa2a0b7eb85c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c23167348946b9afbb3256dc4bb44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679791b080184e8e9dafe44656c62ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83aae449acac454fbf4ced1a6337f971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0cf2d5330e455d92330e2deb3d49f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9365c23389c24f57a68422627ae7ccd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bea207ce604913bf78ed5bbc38d272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributes = [\n",
    "    \"Country\",\n",
    "    \"Continent\",\n",
    "    \"Language\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"Timezone\",\n",
    "]\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    dataset = generate_ravel_dataset(\n",
    "        10000,\n",
    "        root_path=\"/workspace/HyperDAS/assets/data/ravel\",\n",
    "        target_attributes=[\"Country\", \"Continent\"],\n",
    "        isolate_attributes=list(set(all_attributes) - set([\"Country\", \"Continent\"])),\n",
    "        template_split=split,\n",
    "        entity_split=split,\n",
    "    )\n",
    "\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "    dataset.save_to_disk(\n",
    "        f\"/workspace/HyperDAS/experiments/RAVEL/data/city_country_{split}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: {base_entity} â†’ {source_entity} | ATTR: {target_attribute}\n",
    "all_attributes = [\n",
    "    \"Country\",\n",
    "    \"Continent\",\n",
    "    \"Language\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"Timezone\",\n",
    "]\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    dataset = generate_ravel_dataset(\n",
    "        10000,\n",
    "        root_path=\"/workspace/HyperDAS/assets/data/ravel\",\n",
    "        target_attributes=[\"Country\", \"Continent\"],\n",
    "        isolate_attributes=list(set(all_attributes) - set([\"Country\", \"Continent\"])),\n",
    "        template_split=split,\n",
    "        entity_split=split,\n",
    "        edit_instruction_template=\"CHANGE: {base_entity} -> {source_entity} | ATTR: {random_target_attribute}\",\n",
    "    )\n",
    "\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "    dataset.save_to_disk(\n",
    "        f\"/workspace/HyperDAS/experiments/RAVEL/data/city_country_{split}_v0.1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributes = [\n",
    "    \"Country\",\n",
    "    \"Continent\",\n",
    "    \"Language\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"Timezone\",\n",
    "]\n",
    "\n",
    "for attribute in all_attributes:\n",
    "    for split, size in [(\"train\", 20000), (\"test\", 4000)]:\n",
    "        print(f\"Generating data for {attribute} in split {split}\")\n",
    "        all_other_attributes = [a for a in all_attributes if a != attribute]\n",
    "\n",
    "        dataset = generate_ravel_dataset(\n",
    "            size,\n",
    "            root_path=\"/home/ubuntu/HyperDAS/data/ravel\",\n",
    "            target_attributes=[attribute],\n",
    "            isolate_attributes=all_other_attributes,\n",
    "            template_split=split,\n",
    "            entity_split=\"both\",\n",
    "        )\n",
    "\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "        dataset.save_to_disk(\n",
    "            f\"/home/ubuntu/HyperDAS/experiments/ravel/data/city_{attribute.lower()}_{split}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ravel_causal_dataset(split):\n",
    "    dataset = generate_ravel_dataset(\n",
    "        10000,\n",
    "        root_path=\"/workspace/HyperDAS/assets/data/ravel\",\n",
    "        target_attributes=[\"Country\"],\n",
    "        isolate_attributes=[],\n",
    "        template_split=split,\n",
    "        entity_split=split,\n",
    "    )\n",
    "\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "    dataset.save_to_disk(\n",
    "        f\"/workspace/HyperDAS/experiments/RAVEL/data/ravel_country_causal_only_{split}\"\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = (\n",
    "    generate_ravel_causal_dataset(\"train\"),\n",
    "    generate_ravel_causal_dataset(\"test\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_country = [d for d in test_dataset if d[\"attribute\"] == \"Country\"]\n",
    "all_country_with_isolation = [\n",
    "    d\n",
    "    for d in test_dataset\n",
    "    if d[\"attribute\"] == \"Country\" and d[\"attribute_type\"] == \"isolate\"\n",
    "]\n",
    "len(test_dataset), len(all_country), len(all_country_with_isolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_dict = {}\n",
    "\n",
    "for d in test_dataset:\n",
    "    if d[\"attribute\"] not in attr_dict:\n",
    "        attr_dict[d[\"attribute\"]] = 0\n",
    "\n",
    "    attr_dict[d[\"attribute\"]] += 1\n",
    "\n",
    "attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

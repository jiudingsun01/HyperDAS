{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "from src.hyperdas.data_utils import (\n",
    "    filter_dataset,\n",
    "    generate_ravel_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dcc8ee40284fd5b22e9c3f34d14f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.bfloat16)\n",
    "model = model.cuda()\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:50, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5883; filtered out 4117 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "368it [00:27, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9959204487506375; filtered out 24 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "367it [00:25, 14.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9989759344598055; filtered out 6 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7bb8059c5c47dab573a3de3128c4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:45, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5804; filtered out 4196 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "363it [00:25, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9943142660234321; filtered out 33 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "361it [00:24, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9975740772829665; filtered out 14 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dab647f430a4c9389c672bb679873c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_attributes = [\n",
    "    \"Country\",\n",
    "    \"Continent\",\n",
    "    \"Language\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"Timezone\",\n",
    "]\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    dataset = generate_ravel_dataset(\n",
    "        10000,\n",
    "        root_path=\"/workspace/HyperDAS/assets/data/ravel\",\n",
    "        target_attributes=[\"Country\", \"Continent\"],\n",
    "        isolate_attributes=list(set(all_attributes) - set([\"Country\", \"Continent\"])),\n",
    "        template_split=split,\n",
    "        entity_split=split,\n",
    "    )\n",
    "\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "    dataset.save_to_disk(\n",
    "        f\"/workspace/HyperDAS/experiments/RAVEL/data/city_country_{split}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:49, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5956; filtered out 4044 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "373it [00:27, 13.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9971457353928811; filtered out 17 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "372it [00:25, 14.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9988213503956895; filtered out 7 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651a65af480449e2ae0600de9244ce8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5932 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:44, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5884; filtered out 4116 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "368it [00:25, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9954112848402448; filtered out 27 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "367it [00:24, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9994877923851802; filtered out 3 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44dbe01a21248fe83c9086453c6590b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CHANGE: {base_entity} â†’ {source_entity} | ATTR: {target_attribute}\n",
    "all_attributes = [\n",
    "    \"Country\",\n",
    "    \"Continent\",\n",
    "    \"Language\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"Timezone\",\n",
    "]\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    dataset = generate_ravel_dataset(\n",
    "        10000,\n",
    "        root_path=\"/workspace/HyperDAS/assets/data/ravel\",\n",
    "        target_attributes=[\"Country\", \"Continent\"],\n",
    "        isolate_attributes=list(set(all_attributes) - set([\"Country\", \"Continent\"])),\n",
    "        template_split=split,\n",
    "        entity_split=split,\n",
    "        edit_instruction_template=\"CHANGE: {base_entity} -> {source_entity} | ATTR: {random_target_attribute}\",\n",
    "    )\n",
    "\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "    dataset.save_to_disk(\n",
    "        f\"/workspace/HyperDAS/experiments/RAVEL/data/city_country_{split}_v0.1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributes = [\n",
    "    \"Country\",\n",
    "    \"Continent\",\n",
    "    \"Language\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"Timezone\",\n",
    "]\n",
    "\n",
    "for attribute in all_attributes:\n",
    "    for split, size in [(\"train\", 20000), (\"test\", 4000)]:\n",
    "        print(f\"Generating data for {attribute} in split {split}\")\n",
    "        all_other_attributes = [a for a in all_attributes if a != attribute]\n",
    "\n",
    "        dataset = generate_ravel_dataset(\n",
    "            size,\n",
    "            root_path=\"/home/ubuntu/HyperDAS/data/ravel\",\n",
    "            target_attributes=[attribute],\n",
    "            isolate_attributes=all_other_attributes,\n",
    "            template_split=split,\n",
    "            entity_split=\"both\",\n",
    "        )\n",
    "\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "        dataset.save_to_disk(\n",
    "            f\"/home/ubuntu/HyperDAS/experiments/ravel/data/city_{attribute.lower()}_{split}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ravel_causal_dataset(split):\n",
    "    dataset = generate_ravel_dataset(\n",
    "        10000,\n",
    "        root_path=\"/workspace/HyperDAS/assets/data/ravel\",\n",
    "        target_attributes=[\"Country\"],\n",
    "        isolate_attributes=[],\n",
    "        template_split=split,\n",
    "        entity_split=split,\n",
    "    )\n",
    "\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "    dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "    dataset.save_to_disk(\n",
    "        f\"/workspace/HyperDAS/experiments/RAVEL/data/ravel_country_causal_only_{split}\"\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = (\n",
    "    generate_ravel_causal_dataset(\"train\"),\n",
    "    generate_ravel_causal_dataset(\"test\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_country = [d for d in test_dataset if d[\"attribute\"] == \"Country\"]\n",
    "all_country_with_isolation = [\n",
    "    d\n",
    "    for d in test_dataset\n",
    "    if d[\"attribute\"] == \"Country\" and d[\"attribute_type\"] == \"isolate\"\n",
    "]\n",
    "len(test_dataset), len(all_country), len(all_country_with_isolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_dict = {}\n",
    "\n",
    "for d in test_dataset:\n",
    "    if d[\"attribute\"] not in attr_dict:\n",
    "        attr_dict[d[\"attribute\"]] = 0\n",
    "\n",
    "    attr_dict[d[\"attribute\"]] += 1\n",
    "\n",
    "attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

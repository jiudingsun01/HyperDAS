{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "from src.hyperdas.data_utils import (\n",
    "    filter_dataset,\n",
    "    generate_ravel_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Field', 'Award Year', 'Birth Year', 'Country of Birth', 'Gender'])\n"
     ]
    }
   ],
   "source": [
    "p = \"/workspace/HyperDAS/assets/data/ravel/ravel_nobel_prize_winner_attribute_to_prompts.json\"\n",
    "with open(p, \"r\") as f:\n",
    "    prompts = json.load(f)\n",
    "    print(prompts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Duty', 'Gender Bias', 'Industry', 'Work Location'])\n"
     ]
    }
   ],
   "source": [
    "p = \"/workspace/HyperDAS/assets/data/ravel/ravel_occupation_attribute_to_prompts.json\"\n",
    "with open(p, \"r\") as f:\n",
    "    prompts = json.load(f)\n",
    "    print(prompts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6b088d657249cca01eed58607355ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "# CHANGE: {base_entity} → {source_entity} | ATTR: {target_attribute}\n",
    "from collections import defaultdict\n",
    "\n",
    "all_attributes = {\n",
    "    \"city\": [\"Country\", \"Continent\", \"Language\", \"Latitude\", \"Longitude\", \"Timezone\"],\n",
    "    \"nobel_prize_winner\": [\n",
    "        \"Field\",\n",
    "        \"Award Year\",\n",
    "        \"Birth Year\",\n",
    "        \"Country of Birth\",\n",
    "        \"Gender\",\n",
    "    ],\n",
    "    \"occupation\": [\"Field\", \"Award Year\", \"Birth Year\", \"Country of Birth\", \"Gender\"],\n",
    "}\n",
    "\n",
    "target_attributes = {\n",
    "    \"city\": [\"Country\"],\n",
    "    \"nobel_prize_winner\": [\"Field\"],\n",
    "    \"occupation\": [\"Duty\"],\n",
    "}\n",
    "\n",
    "\n",
    "domains = [\"city\", \"nobel_prize_winner\", \"occupation\"]\n",
    "all_datasets = defaultdict(list)\n",
    "\n",
    "for domain in domains:\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        args = {\n",
    "            \"n_samples\": 10000,\n",
    "            \"root_path\": \"/workspace/HyperDAS/assets/data/ravel\",\n",
    "            \"target_attributes\": target_attributes[domain],\n",
    "            \"isolate_attributes\": list(\n",
    "                set(all_attributes[domain]) - set(target_attributes[domain])\n",
    "            ),\n",
    "            \"template_split\": split,\n",
    "            \"entity_split\": split,\n",
    "            \"domain\": domain,\n",
    "            # \"edit_instruction_template\": \"CHANGE: {base_entity} -> {source_entity} | ATTR: {random_target_attribute}\",\n",
    "        }\n",
    "\n",
    "        dataset = generate_ravel_dataset(**args)\n",
    "\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "        metadata = {\n",
    "            **args,\n",
    "            \"target_attributes\": tuple(args[\"target_attributes\"]),\n",
    "            \"isolate_attributes\": tuple(args[\"isolate_attributes\"]),\n",
    "        }\n",
    "\n",
    "        all_datasets[split].append((dataset, metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset_list in all_datasets.items():\n",
    "    dataset_list, metadata_list = zip(*dataset_list)\n",
    "    combined = datasets.concatenate_datasets(dataset_list)\n",
    "    path = f\"/workspace/HyperDAS/experiments/RAVEL/data/city_nobel_prize_winner_occupation_{split}\"\n",
    "    combined.save_to_disk(path)\n",
    "    with open(os.path.join(path, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump({\"metadata\": metadata_list}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: {base_entity} → {source_entity} | ATTR: {target_attribute}\n",
    "from collections import defaultdict\n",
    "\n",
    "all_attributes = {\n",
    "    \"city\": [\"Country\", \"Continent\", \"Language\", \"Latitude\", \"Longitude\", \"Timezone\"],\n",
    "    \"nobel_prize_winner\": [\n",
    "        \"Field\",\n",
    "        \"Award Year\",\n",
    "        \"Birth Year\",\n",
    "        \"Country of Birth\",\n",
    "        \"Gender\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "target_attributes = {\n",
    "    \"city\": [\"Country\"],\n",
    "    \"nobel_prize_winner\": [\"Field\"],\n",
    "}\n",
    "\n",
    "\n",
    "domains = [\"city\", \"nobel_prize_winner\"]\n",
    "all_datasets = defaultdict(list)\n",
    "\n",
    "for domain in domains:\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        args = {\n",
    "            \"n_samples\": 10000,\n",
    "            \"root_path\": \"/workspace/HyperDAS/assets/data/ravel\",\n",
    "            \"target_attributes\": target_attributes[domain],\n",
    "            \"isolate_attributes\": list(\n",
    "                set(all_attributes[domain]) - set(target_attributes[domain])\n",
    "            ),\n",
    "            \"template_split\": split,\n",
    "            \"entity_split\": split,\n",
    "            \"domain\": domain,\n",
    "            # \"edit_instruction_template\": \"CHANGE: {base_entity} -> {source_entity} | ATTR: {random_target_attribute}\",\n",
    "        }\n",
    "\n",
    "        dataset = generate_ravel_dataset(**args)\n",
    "\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "        dataset = filter_dataset(model, tokenizer, dataset, batch_size=16)\n",
    "\n",
    "        metadata = {\n",
    "            **args,\n",
    "            \"target_attributes\": tuple(args[\"target_attributes\"]),\n",
    "            \"isolate_attributes\": tuple(args[\"isolate_attributes\"]),\n",
    "        }\n",
    "\n",
    "        all_datasets[split].append((dataset, metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset_list in all_datasets.items():\n",
    "    dataset_list, metadata_list = zip(*dataset_list)\n",
    "    combined = datasets.concatenate_datasets(dataset_list)\n",
    "    path = f\"/workspace/HyperDAS/experiments/RAVEL/data/city_nobel_prize_winner_{split}\"\n",
    "    combined.save_to_disk(path)\n",
    "    with open(os.path.join(path, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump({\"metadata\": metadata_list}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
